  <page>
    <title>AVL tree</title>
    <ns>0</ns>
    <id>2118</id>
    <revision>
      <id>939147493</id>
      <parentid>938689450</parentid>
      <timestamp>2020-02-04T17:37:29Z</timestamp>
      <contributor>
        <ip>188.100.197.70</ip>
      </contributor>
      <comment>/* Insert */  merge if/else tails</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve">{{Infobox data structure
|name=AVL tree
|type=tree
|invented_by=[[Georgy Adelson-Velsky]] and [[Evgenii Landis]]
|invented_year=1962
|
|space_avg=&lt;math&gt;O(n)&lt;/math&gt;
|space_worst=&lt;math&gt;O(n)&lt;/math&gt;
|search_avg=&lt;math&gt;O(\log n)&lt;/math&gt;&lt;ref name=&quot;wiscurl&quot;&gt;{{cite web | url=http://pages.cs.wisc.edu/~ealexand/cs367/NOTES/AVL-Trees/index.html | title=AVL Trees | author=Eric Alexander}}&lt;/ref&gt;
|search_worst=&lt;math&gt;O(\log n)&lt;/math&gt;&lt;ref name=&quot;wiscurl&quot;/&gt;
|insert_avg=&lt;math&gt;O(\log n)&lt;/math&gt;&lt;ref name=&quot;wiscurl&quot;/&gt;
|insert_worst=&lt;math&gt;O(\log n)&lt;/math&gt;&lt;ref name=&quot;wiscurl&quot;/&gt;
|delete_avg=&lt;math&gt;O(\log n)&lt;/math&gt;&lt;ref name=&quot;wiscurl&quot;/&gt;
|delete_worst=&lt;math&gt;O(\log n)&lt;/math&gt;&lt;ref name=&quot;wiscurl&quot;/&gt;
}}
[[File:AVL Tree Example.gif|thumb|Animation showing the insertion of several elements into an AVL tree. It includes left, right, left-right and right-left rotations.]]
[[Image:AVL-tree-wBalance_K.svg|thumb|right|262px|Fig. 1: AVL tree with balance factors (green)]]

In [[computer science]], an '''AVL tree''' (named after inventors '''A'''delson-'''V'''elsky and '''L'''andis) is a [[self-balancing binary search tree]]. It was the first such [[data structure]] to be invented.&lt;ref&gt;{{cite book|last=Sedgewick|first=Robert|authorlink1=Robert Sedgewick (computer scientist)|title=Algorithms|publisher=Addison-Wesley|year=1983|isbn=0-201-06672-6|p=[https://archive.org/details/algorithms00sedg/page/199 199]|chapter=Balanced Trees|url-access=registration|url=https://archive.org/details/algorithms00sedg/page/199}}&lt;/ref&gt; In an AVL tree, the [[tree height|heights]] of the two [[child node|child]] subtrees of any node differ by at most one; if at any time they differ by more than one, rebalancing is done to restore this property. Lookup, insertion, and deletion all take {{math|[[big O notation|O]](log ''n'')}} time in both the average and worst cases, where &lt;math&gt;n&lt;/math&gt; is the number of nodes in the tree prior to the operation. Insertions and deletions may require the tree to be rebalanced by one or more [[tree rotation]]s.

The AVL tree is named after its two [[Soviet Union|Soviet]] inventors, [[Georgy Adelson-Velsky]] and [[Evgenii Landis]], who published it in their 1962 paper &quot;An algorithm for the organization of information&quot;.&lt;ref&gt;{{cite journal|last1=Adelson-Velsky|first1=Georgy|last2=Landis|first2=Evgenii|year=1962|title=An algorithm for the organization of information|journal=[[Proceedings of the USSR Academy of Sciences]]|volume=146|pages=263–266|language=ru}} [http://professor.ufabc.edu.br/~jesus.mena/courses/mc3305-2q-2015/AED2-10-avl-paper.pdf English translation] by Myron J. Ricci in ''Soviet Mathematics - Doklady'', 3:1259–1263, 1962.&lt;/ref&gt;

AVL trees are often compared with [[red–black tree]]s because both support the same set of operations and take &lt;math&gt;O(\log n)&lt;/math&gt; time for the basic operations. For lookup-intensive applications, AVL trees are faster than red–black trees because they are more strictly balanced.&lt;ref&gt;{{cite web|last = Pfaff|first = Ben|title = Performance Analysis of BSTs in System Software| publisher = [[Stanford university|Stanford University]]|date=June 2004|url = http://www.stanford.edu/~blp/papers/libavl.pdf|format = PDF}}&lt;/ref&gt; Similar to red–black trees, AVL trees are height-balanced. Both are, in general, neither [[Weight-balanced tree|weight-balanced]] nor &lt;math&gt;\mu&lt;/math&gt;-balanced for any &lt;math&gt;\mu\leq\tfrac{1}{2}&lt;/math&gt;;&lt;ref&gt;[https://cs.stackexchange.com/q/421 AVL trees are not weight-balanced? (meaning: AVL trees are not μ-balanced?)] &lt;br /&gt;Thereby: A Binary Tree is called &lt;math&gt;\mu&lt;/math&gt;-balanced, with &lt;math&gt;0 \le\mu\leq\tfrac12&lt;/math&gt;, if for every node &lt;math&gt;N&lt;/math&gt;, the inequality
: &lt;math&gt;\tfrac12-\mu\le\tfrac{|N_l|}{|N|+1}\le \tfrac12+\mu&lt;/math&gt;
holds and &lt;math&gt;\mu&lt;/math&gt; is minimal with this property. &lt;math&gt;|N|&lt;/math&gt; is the number of nodes below the tree with &lt;math&gt;N&lt;/math&gt; as root (including the root) and &lt;math&gt;N_l&lt;/math&gt; is the left child node of &lt;math&gt;N&lt;/math&gt;.&lt;/ref&gt; that is, sibling nodes can have hugely differing numbers of descendants.

==Definition==

===Balance factor===
In a [[binary tree]] the ''balance factor'' of a node &lt;math&gt;N&lt;/math&gt; is defined to be the height difference

:&lt;math&gt; \text{BalanceFactor}(N) := \text{Height}(\text{RightSubtree}(N)) - \text{Height}(\text{LeftSubtree}(N)) &lt;/math&gt;&lt;ref&gt;{{cite book|last=Knuth|first=Donald E.|title=Sorting and searching|year=2000|publisher=Addison-Wesley|location=Boston [u.a.]|isbn=0-201-89685-0|pages=459|edition=2. ed., 6. printing, newly updated and rev.}}&lt;/ref&gt;

of its two child sub-trees. A binary tree is defined to be an ''AVL tree'' if the [[Invariant (computer science)|invariant]]

:&lt;math&gt;\text{BalanceFactor}(N) \in {\{-1,0,1\}}&lt;/math&gt;&lt;ref&gt;{{Cite web|url=http://www.btechsmartclass.com/data_structures/avl-trees.html|title=AVL Tree :: Data Structures|last=Rajinikanth|website=btechsmartclass.com|access-date=2018-03-09}}&lt;/ref&gt;

holds for every node &lt;math&gt;N&lt;/math&gt; in the tree.

A node &lt;math&gt;N&lt;/math&gt; with &lt;math&gt;\text{BalanceFactor}(N)&lt;0&lt;/math&gt; is called &quot;left-heavy&quot;, one with &lt;math&gt;\text{BalanceFactor}(N)&gt;0&lt;/math&gt; is called &quot;right-heavy&quot;, and one with &lt;math&gt;\text{BalanceFactor}(N)=0&lt;/math&gt; is sometimes simply called &quot;balanced&quot;.

;Remark

In what follows, because there is a one-to-one correspondence between nodes and the sub-trees rooted by them, the name of an object is sometimes used to refer to the node and sometimes used to refer to the sub-tree.

===Properties===
Balance factors can be kept up-to-date by knowing the previous balance factors and the change in height – it is not necessary to know the absolute height. For holding the AVL balance information in the traditional way, two bits per node are sufficient. However, later research showed if the AVL tree is implemented as a rank balanced tree with delta ranks allowed of 1 or 2 – with meaning &quot;when going upward there is an additional increment in height of one or two&quot;, this can be done with one bit.

The height {{math|''h''}} (counted as number of edges on the longest path) of an AVL tree with {{math|''n''}} nodes lies in the interval:&lt;ref&gt;{{cite book|last=Knuth|first=Donald E.|title=Sorting and searching|year=2000|publisher=Addison-Wesley|location=Boston [u.a.]|isbn=0-201-89685-0|pages=460|edition=2. ed., 6. printing, newly updated and rev.}}&lt;br /&gt;Knuth has internal nodes and external nodes, the first ones correspond to the article's key carrying nodes, whereas Knuth's external nodes (which do not carry a key) have no correspondence in the article. Nevertheless Knuth's external nodes increase the tree's height by 1 (see Fig. 20), an incrementation which the article does not follow. At the end with the article's notion of height, the tree consisting of the root only has height 0, so that {{math|''F''&lt;sub&gt;0+2&lt;/sub&gt; – 1 &amp;#61; 1}} is the number of its nodes.&lt;br /&gt;NB: &lt;math&gt;\lim_{h\to\infty}F_h \sqrt5 \, \varphi^{-h} = 1 &lt;/math&gt;.&lt;/ref&gt;
:&lt;math&gt;\log_2(n+1) - 1 \le h &lt; c \log_2(n+2) + b&lt;/math&gt; 
with the [[golden ratio]] {{math|&amp;phi; :&amp;#61; (1+}}&lt;span class=&quot;nowrap&quot;&gt;&amp;radic;&lt;span style=&quot;border-top:1px solid; padding:0 0.1em;&quot;&gt;{{math|5}}&lt;/span&gt;{{math|) &amp;frasl;&lt;sub&gt;2&lt;/sub&gt; &amp;asymp; 1.6180}}, {{math|''c'' :&amp;#61; &lt;sup&gt;1&lt;/sup&gt;&amp;frasl; log&lt;sub&gt;2&lt;/sub&gt; &amp;phi; &amp;asymp; 1.4405}}&lt;/span&gt;, &amp;nbsp;and&amp;nbsp; {{math|''b'' :&amp;#61; &lt;sup&gt;''c''&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt; log&lt;sub&gt;2&lt;/sub&gt; 5 – 3 &amp;asymp; –1.3277}}.
This is because an AVL tree of height {{math|''h''}} contains at least {{math|''F''&lt;sub&gt;''h''+2&lt;/sub&gt; – 1}} nodes where {{math|&amp;#123;''F''&lt;sub&gt;''h''&lt;/sub&gt;&amp;#125;}} is the [[Fibonacci number|Fibonacci sequence]] with the seed values {{math|''F''&lt;sub&gt;''1''&lt;/sub&gt; &amp;#61; 1}}, {{math|''F''&lt;sub&gt;''2''&lt;/sub&gt; &amp;#61; 1}}.

==Operations==
Read-only operations of an AVL tree involve carrying out the same actions as would be carried out on an unbalanced [[binary search tree]], but modifications have to observe and restore the height balance of the sub-trees.

===Searching===
Searching for a specific key in an AVL tree can be done the same way as that of any balanced or unbalanced [[Binary search tree#Searching|binary search tree]].&lt;ref name=&quot;dixit-mastering-data-structures&quot;&gt;{{Cite book|url=https://www.worldcat.org/oclc/939446542|title=Mastering data structures through 'C' language|last=Dixit|first=J. B.|publisher=University Science Press, an imprint of Laxmi Publications Pvt. Ltd.|year=2010|isbn=9789380386720|location=New Delhi, India|oclc=939446542}}&lt;/ref&gt;{{rp|ch. 8}} In order for search to work effectively it has to employ a comparison function which establishes a [[total order]] (or at least a [[Weak ordering#Total preorders|total preorder]]) on the set of keys.&lt;ref name=&quot;brass-advanced-data-structures&quot;&gt;{{Cite book|url=https://www.worldcat.org/oclc/312435417|title=Advanced data structures|last=Brass|first=Peter|date=2008|publisher=Cambridge University Press|year=|isbn=9780511438202|location=Cambridge|oclc=312435417}}&lt;/ref&gt;{{rp|23}} The number of comparisons required for successful search is limited by the height {{math|''h''}} and for unsuccessful search is very close to {{math|''h''}}, so both are in {{math|O(log ''n'')}}.&lt;ref name=&quot;hubbard-outline-theory-problems&quot;&gt;{{Cite book|url=https://archive.org/details/schaumsoutlineof0000hubb|title=Schaum's outline of theory and problems of data structures with Java|last=Hubbard|first=John Rast|date=2000|publisher=McGraw-Hill|year=|isbn=0071378707|location=New York|pages=|oclc=48139308|url-access=registration}}&lt;/ref&gt;{{rp|216}}

===Traversal===
{{More citations needed section|date=July 2016}}
Once a node has been found in an AVL tree, the ''next'' or ''previous'' node can be accessed in [[amortized complexity|amortized]] constant time. Some instances of exploring these &quot;nearby&quot; nodes require traversing up to {{math|''h'' &amp;prop; log(''n'')}} links (particularly when navigating from the rightmost leaf of the root's left subtree to the root or from the root to the leftmost leaf of the root's right subtree; in the AVL tree of figure 1, moving from node P to the ''next but one'' node Q takes 3 steps). However, exploring all {{math|''n''}} nodes of the tree in this manner would visit each link exactly twice: one downward visit to enter the subtree rooted by that node, another visit upward to leave that node's subtree after having explored it. And since there are {{math|''n''−1}} links in any tree, the amortized cost is {{math|2×(''n''−1)/''n''}}, or approximately 2.

===Insert===
{{Multiple issues|section=yes|
{{Expand section|date=November 2016|small=no}}
{{More citations needed section|date=November 2016}}
}}
When inserting an element into an AVL tree, you initially follow the same process as inserting into a [[Binary Search Tree]].  More explicitly: In case a preceding search has not been successful the search routine returns the tree itself with indication EMPTY and the new node is inserted as root. Or, if the tree has not been empty the search routine returns a node and a direction (left or right) where the returned node does not have a child. Then the node to be inserted is made child of the returned node at the returned direction.

After this insertion it is necessary to check each of the node's ancestors for consistency with the invariants of AVL trees: this is called &quot;retracing&quot;. This is achieved by considering the [[#Balance factor|balance factor]] of each node.&lt;ref name=&quot;Sorting and searching&quot;&gt;{{cite book|last1=Knuth|first1=Donald E.|title=Sorting and searching|date=2000|publisher=Addison-Wesley|location=Boston [u.a.]|isbn=0201896850|pages=458–481|edition=2. ed., 6. printing, newly updated and rev.}}&lt;/ref&gt;&lt;ref name=&quot;Free Software Foundation, Inc&quot;&gt;{{cite book|last1=Pfaff|first1=Ben|title=An Introduction to Binary Search Trees and Balanced Trees|date=2004|publisher=Free Software Foundation, Inc.|pages=107–138}}&lt;/ref&gt;

Since with a single insertion the height of an AVL subtree cannot increase by more than one, the temporary balance factor of a node after an insertion will be in the range {{nowrap|[–2,+2].}} For each node checked, if the temporary balance factor remains in the range from –1 to +1 then only an update of the balance factor and no rotation is necessary. However, if the temporary balance factor becomes less than –1 or greater than +1, the subtree rooted at this node is AVL unbalanced, and a rotation is needed.&lt;ref name=&quot;brass-advanced-data-structures&quot; /&gt;{{rp|52}}  With insertion as the code below shows, the adequate rotation immediately perfectly [[#Rebalancing|rebalances]] the tree.

In figure 1, by inserting the new node Z as a child of node X the height of that subtree Z increases from 0 to 1.

;[[Loop invariant|Invariant]] of the retracing loop for an insertion
The height of the subtree rooted by Z has increased by 1. It is already in AVL shape.
{{Collapse top|Example code for an insert operation}}
&lt;source lang=&quot;c&quot; style=&quot;overflow:hidden&quot; line=&quot;1&quot;&gt;
for (X = parent(Z); X != null; X = parent(Z)) { // Loop (possibly up to the root)
    // BalanceFactor(X) has to be updated:
    if (Z == right_child(X)) { // The right subtree increases
        if (BalanceFactor(X) &gt; 0) { // X is right-heavy
            // ===&gt; the temporary BalanceFactor(X) == +2
            // ===&gt; rebalancing is required.
            G = parent(X); // Save parent of X around rotations
            if (BalanceFactor(Z) &lt; 0)      // Right Left Case     (see figure 5)
                N = rotate_RightLeft(X, Z); // Double rotation: Right(Z) then Left(X)
            else                           // Right Right Case    (see figure 4)
                N = rotate_Left(X, Z);     // Single rotation Left(X)
            // After rotation adapt parent link
        } else {
            if (BalanceFactor(X) &lt; 0) {
                BalanceFactor(X) = 0; // Z’s height increase is absorbed at X.
                break; // Leave the loop
            }
            BalanceFactor(X) = +1;
            Z = X; // Height(Z) increases by 1
            continue;
        }
    } else { // Z == left_child(X): the left subtree increases
        if (BalanceFactor(X) &lt; 0) { // X is left-heavy
            // ===&gt; the temporary BalanceFactor(X) == –2
            // ===&gt; rebalancing is required.
            G = parent(X); // Save parent of X around rotations
            if (BalanceFactor(Z) &gt; 0)      // Left Right Case
                N = rotate_LeftRight(X, Z); // Double rotation: Left(Z) then Right(X)
            else                           // Left Left Case
                N = rotate_Right(X, Z);    // Single rotation Right(X)
            // After rotation adapt parent link
        } else {
            if (BalanceFactor(X) &gt; 0) {
                BalanceFactor(X) = 0; // Z’s height increase is absorbed at X.
                break; // Leave the loop
            }
            BalanceFactor(X) = –1;
            Z = X; // Height(Z) increases by 1
            continue;
        }
    }
    // After a rotation adapt parent link:
    // N is the new root of the rotated subtree
    // Height does not change: Height(N) == old Height(X)
    parent(N) = G;
    if (G != null) {
        if (X == left_child(G))
            left_child(G) = N;
        else
            right_child(G) = N;
    } else
        tree-&gt;root = N; // N is the new root of the total tree
    break;
    // There is no fall thru, only break; or continue;
}
// Unless loop is left via break, the height of the total tree increases by 1.
&lt;/source&gt;
{{Collapse bottom}}
In order to update the balance factors of all nodes, first observe that all nodes requiring correction lie from child to parent along the path of the inserted leaf. If the above procedure is applied to nodes along this path, starting from the leaf, then every node in the tree will again have a balance factor of −1, 0, or 1.

The retracing can stop if the balance factor becomes 0 implying that the height of that subtree remains unchanged.

If the balance factor becomes ±1 then the height of the subtree increases by one and the retracing needs to continue.

If the balance factor temporarily becomes ±2, this has to be repaired by an appropriate rotation after which the subtree has the same height as before (and its root the balance factor 0).

The time required is {{math|O(log ''n'')}} for lookup, plus a maximum of {{math|O(log ''n'')}} retracing levels ({{math|O(1)}} on average) on the way back to the root, so the operation can be completed in {{math|O(log ''n'')}} time.&lt;ref name=&quot;brass-advanced-data-structures&quot; /&gt;{{rp|53}}

===Delete===
The preliminary steps for deleting a node are described in section [[Binary search tree#Deletion]].
There, the effective deletion of the subject node or the replacement node decreases the height of the corresponding child tree either from 1 to 0 or from 2 to 1, if that node had a child.

Starting at this subtree, it is necessary to check each of the ancestors for consistency with the invariants of AVL trees. This is called &quot;retracing&quot;.

Since with a single deletion the height of an AVL subtree cannot decrease by more than one, the temporary balance factor of a node will be in the range from −2 to +2.
If the balance factor remains in the range from −1 to +1 it can be adjusted in accord with the AVL rules. If it becomes ±2 then the subtree is unbalanced and needs to be rotated. (Unlike insertion where a rotation always balances the tree, after delete, there may be BF(Z) ≠ 0 (see fig.s 4 and 5), so that after the appropriate single or double rotation the height of the rebalanced subtree decreases by one meaning that the tree has to be rebalanced again on the next higher level.) The various cases of rotations are described in section [[#Rebalancing|Rebalancing]].

;Invariant of the retracing loop for a deletion
The height of the subtree rooted by N has decreased by 1. It is already in AVL shape.
{{Collapse top|Example code for a delete operation}}
&lt;source lang=&quot;c&quot; style=&quot;overflow:hidden&quot; line=&quot;1&quot;&gt;
for (X = parent(N); X != null; X = G) { // Loop (possibly up to the root)
    G = parent(X); // Save parent of X around rotations
    // BalanceFactor(X) has not yet been updated!
    if (N == left_child(X)) { // the left subtree decreases
        if (BalanceFactor(X) &gt; 0) { // X is right-heavy
            // ===&gt; the temporary BalanceFactor(X) == +2
            // ===&gt; rebalancing is required.
            Z = right_child(X); // Sibling of N (higher by 2)
            b = BalanceFactor(Z);
            if (b &lt; 0)                     // Right Left Case     (see figure 5)
                N = rotate_RightLeft(X, Z); // Double rotation: Right(Z) then Left(X)
            else                           // Right Right Case    (see figure 4)
                N = rotate_Left(X, Z);     // Single rotation Left(X)
            // After rotation adapt parent link
        } else {
            if (BalanceFactor(X) == 0) {
                BalanceFactor(X) = +1; // N’s height decrease is absorbed at X.
                break; // Leave the loop
            }
            N = X;
            BalanceFactor(N) = 0; // Height(N) decreases by 1
            continue;
        }
    } else { // (N == right_child(X)): The right subtree decreases
        if (BalanceFactor(X) &lt; 0) { // X is left-heavy
            // ===&gt; the temporary BalanceFactor(X) == –2
            // ===&gt; rebalancing is required.
            Z = left_child(X); // Sibling of N (higher by 2)
            b = BalanceFactor(Z);
            if (b &gt; 0)                     // Left Right Case
                N = rotate_LeftRight(X, Z); // Double rotation: Left(Z) then Right(X)
            else                        // Left Left Case
                N = rotate_Right(X, Z);    // Single rotation Right(X)
            // After rotation adapt parent link
        } else {
            if (BalanceFactor(X) == 0) {
                BalanceFactor(X) = –1; // N’s height decrease is absorbed at X.
                break; // Leave the loop
            }
            N = X;
            BalanceFactor(N) = 0; // Height(N) decreases by 1
            continue;
        }
    }
    // After a rotation adapt parent link:
    // N is the new root of the rotated subtree
    parent(N) = G;
    if (G != null) {
        if (X == left_child(G))
            left_child(G) = N;
        else
            right_child(G) = N;
        if (b == 0)
            break; // Height does not change: Leave the loop
    } else {
        tree-&gt;root = N; // N is the new root of the total tree
    }
    // Height(N) decreases by 1 (== old Height(X)-1)
}
// Unless loop is left via break, the height of the total tree decreases by 1.
&lt;/source&gt;
{{Collapse bottom}}
The retracing can stop if the balance factor becomes ±1 (it must have been 0) meaning that the height of that subtree remains unchanged.

If the balance factor becomes 0 (it must have been ±1) then the height of the subtree decreases by one and the retracing needs to continue.

If the balance factor temporarily becomes ±2, this has to be repaired by an appropriate rotation. It depends on the balance factor of the sibling Z (the higher child tree in fig. 4) whether the height of the subtree decreases by one –and the retracing needs to continue– or does not change (if Z has the balance factor 0) and the whole tree is in AVL-shape.

The time required is {{math|O(log ''n'')}} for lookup, plus a maximum of {{math|O(log ''n'')}} retracing levels ({{math|O(1)}} on average) on the way back to the root, so the operation can be completed in {{math|O(log ''n'')}} time.

===Set operations and bulk operations===
In addition to the single-element insert, delete and lookup operations, several set operations have been defined on AVL trees: [[Union (set theory)|union]], [[Intersection (set theory)|intersection]] and [[set difference]]. Then fast ''bulk'' operations on insertions or deletions can be implemented based on these set functions. These set operations rely on two helper operations, ''Split'' and ''Join''. With the new operations, the implementation of AVL trees can be more efficient and highly-parallelizable.&lt;ref name=&quot;join-based&quot;&gt;{{citation
 | last1 = Blelloch | first1 = Guy E.
 | last2 = Ferizovic | first2 = Daniel
 | last3 = Sun | first3 = Yihan
 | chapter = Just join for parallel ordered sets
 | doi = 10.1145/2935764.2935768
 | isbn = 978-1-4503-4210-0
 | pages = 253–264
 | publisher = ACM
 | title = Symposium on Parallel Algorithms and Architectures
 | year = 2016| url = https://arxiv.org/pdf/1602.02120| arxiv = 1602.02120
 }}.&lt;/ref&gt;

*''Join'': The function ''Join'' is on two AVL trees {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}} and a key {{mvar|k}} will return a tree containing all elements in {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}}, {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}} as well as {{mvar|k}}. It requires {{mvar|k}} to be greater than all keys in {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}} and smaller than all keys in {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}}. If the two trees differ by height at most one, ''Join'' simply create a new node with left subtree {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}}, root {{mvar|k}} and right subtree {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}}. Otherwise, suppose that {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}} is higher than {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}} for more than one (the other case is symmetric). ''Join'' follows the right spine of {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}} until a node {{mvar|c}} which is balanced with {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}}. At this point a new node with left child {{mvar|c}}, root {{mvar| k}} and right child {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}} is created to replace c. The new node satisfies the AVL invariant, and its height is one greater than {{mvar|c}}. The increase in height can increase the height of its ancestors, possibly invalidating the AVL invariant of those nodes. This can be fixed either with a double rotation if invalid at the parent or a single left rotation if invalid higher in the tree, in both cases restoring the height for any further ancestor nodes. ''Join'' will therefore require at most two rotations. The cost of this function is the difference of the heights between the two input trees.
{{Collapse top|Pseudocode implementation for the join algorithm}}
 '''function''' joinRightAVL(T&lt;sub&gt;L&lt;/sub&gt;, k, T&lt;sub&gt;R&lt;/sub&gt;)
     (l,k',c)=expose(T&lt;sub&gt;L&lt;/sub&gt;)
     '''if''' (h(c)&lt;h(T&lt;sub&gt;R&lt;/sub&gt;)+1)
        T'=Node(c,k,T&lt;sub&gt;R&lt;/sub&gt;)
        if (h(T')&lt;=h(l)+1) then '''return''' Node(l,k',T')
        else '''return''' rotateLeft(Node(l,k'rotateRight(T')))
     '''else''' 
         T'=joinRightAVL(c,k,T&lt;sub&gt;R&lt;/sub&gt;)
         T''=Node(l,k',T')
         '''if''' (h(T&lt;sub&gt;L&lt;/sub&gt;)&gt;h(T&lt;sub&gt;R&lt;/sub&gt;)+1) '''return''' T''
         '''else''' '''return''' rotateLeft(T'')
 '''function''' joinLeftAVL(T&lt;sub&gt;L&lt;/sub&gt;, k, T&lt;sub&gt;R&lt;/sub&gt;)
   /* symmetric to joinRightAVL */
 '''function''' join(T&lt;sub&gt;L&lt;/sub&gt;, k, T&lt;sub&gt;R&lt;/sub&gt;)
     '''if''' (h(T&lt;sub&gt;L&lt;/sub&gt;)&gt;h(T&lt;sub&gt;R&lt;/sub&gt;)+1) '''return''' joinRightAVL(T&lt;sub&gt;L&lt;/sub&gt;, k, T&lt;sub&gt;R&lt;/sub&gt;)
     '''if''' (h(T&lt;sub&gt;R&lt;/sub&gt;)&gt;h(T&lt;sub&gt;L&lt;/sub&gt;)+1) '''return''' joinLeftAVL(T&lt;sub&gt;L&lt;/sub&gt;, k, T&lt;sub&gt;R&lt;/sub&gt;)
     '''return''' Node(T&lt;sub&gt;L&lt;/sub&gt;,k,T&lt;sub&gt;R&lt;/sub&gt;)
     
Here &lt;math&gt;h(v)&lt;/math&gt; of a node &lt;math&gt;v&lt;/math&gt; the height of &lt;math&gt;v&lt;/math&gt;. expose(v)=(l,k,r) means to extract a tree node &lt;math&gt;v&lt;/math&gt;'s left child &lt;math&gt;l&lt;/math&gt;, the key of the node &lt;math&gt;k&lt;/math&gt;, and the right child &lt;math&gt;r&lt;/math&gt;. Node(l,k,r) means to create a node of left child &lt;math&gt;l&lt;/math&gt;, key &lt;math&gt;k&lt;/math&gt;, and right child &lt;math&gt;r&lt;/math&gt;.
{{Collapse bottom}}

*''Split'': To split an AVL tree into two smaller trees, those smaller than key ''x'', and those larger than key ''x'', first draw a path from the root by inserting ''x'' into the AVL. After this insertion, all values less than ''x'' will be found on the left of the path, and all values greater than ''x'' will be found on the right. By applying ''Join'', all the subtrees on the left side are merged bottom-up using keys on the path as intermediate nodes from bottom to top to form the left tree, and the right part is asymmetric. The cost of ''Split'' is &lt;math&gt;O(\log n)&lt;/math&gt;, order of the height of the tree.
{{Collapse top|Pseudocode implementation for the split algorithm}}
 '''function''' split(T,k)
     '''if''' (T=nil) return (nil,false,nil)
     (L,(m,c),R)=expose(T)
     '''if''' (k=m) return (L,true,R)
     '''if''' (k&lt;m) 
        (L',b,R')=split(L,k)
        '''return''' (L',b,join(R',m,R))
     '''if''' (k&gt;m) 
        (L',b,R')=split(R,k)
        '''return''' (join(L,m,L'),b,R))
{{Collapse bottom}}

The union of two AVLs {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''t''&lt;sub&gt;2&lt;/sub&gt;}} representing sets {{mvar|A}} and {{mvar|B}}, is an AVL {{mvar|''t''}} that represents {{math|''A'' ∪ ''B''}}.

{{Collapse top|Pseudocode implementation for the union algorithm}}
 '''function''' union(t&lt;sub&gt;1&lt;/sub&gt;, t&lt;sub&gt;2&lt;/sub&gt;):
     '''if''' t&lt;sub&gt;1&lt;/sub&gt; = nil:
         '''return''' t&lt;sub&gt;2&lt;/sub&gt;
     '''if''' t&lt;sub&gt;2&lt;/sub&gt; = nil:
         '''return''' t&lt;sub&gt;1&lt;/sub&gt;
     t&lt;sub&gt;&lt;&lt;/sub&gt;, t&lt;sub&gt;&gt;&lt;/sub&gt; ← split t&lt;sub&gt;2&lt;/sub&gt; on t&lt;sub&gt;1&lt;/sub&gt;.root
     '''return''' join(t&lt;sub&gt;1&lt;/sub&gt;.root,union(left(t&lt;sub&gt;1&lt;/sub&gt;), t&lt;sub&gt;&lt;&lt;/sub&gt;),union(right(t&lt;sub&gt;1&lt;/sub&gt;), t&lt;sub&gt;&gt;&lt;/sub&gt;))

Here, ''Split'' is presumed to return two trees: one holding the keys less its input key, one holding the greater keys. (The algorithm is [[persistent data structure|non-destructive]], but an in-place destructive version exists as well.)
{{Collapse bottom}}

The algorithm for intersection or difference is similar, but requires the ''Join2'' helper routine that is the same as ''Join'' but without the middle key. Based on the new functions for union, intersection or difference, either one key or multiple keys can be inserted to or deleted from the AVL tree. Since ''Split'' calls ''Join'' but does not deal with the balancing criteria of AVL trees directly, such an implementation is usually called the [[Join-based tree algorithms|&quot;join-based&quot; implementation]].

The complexity of each of union, intersection and difference is &lt;math&gt;O\left(m \log \left({n\over m}+1\right)\right)&lt;/math&gt; for AVLs of sizes &lt;math&gt;m&lt;/math&gt; and &lt;math&gt;n(\ge m)&lt;/math&gt;. More importantly, since the recursive calls to union, intersection or difference are independent of each other, they can be executed [[parallel programming|in parallel]] with a [[Analysis of parallel algorithms|parallel depth]] &lt;math&gt;O(\log m\log n)&lt;/math&gt;.&lt;ref name=&quot;join-based&quot;/&gt; When &lt;math&gt;m=1&lt;/math&gt;, the join-based implementation has the same computational DAG as single-element insertion and deletion.

==Rebalancing==
If during a modifying operation (e.g. insert, delete) a (temporary) height difference of more than one arises between two child subtrees, the parent subtree has to be &quot;rebalanced&quot;. The given repair tools are the so-called [[tree rotation]]s, because they move the keys only &quot;vertically&quot;, so that the (&quot;horizontal&quot;) in-order sequence of the keys is fully preserved (which is essential for a binary-search tree).&lt;ref name=&quot;Sorting and searching&quot;/&gt;&lt;ref name=&quot;Free Software Foundation, Inc&quot;/&gt;

Let X be the node that has a (temporary) balance factor of −2 or +2. Its left or right subtree was modified. Let Z be the higher child. Note that Z is in AVL shape by [[Mathematical induction|induction hypothesis]].

In case of insertion this insertion has happened to one of Z's children in a way that Z's height has increased.
In case of deletion this deletion has happened to the sibling t&lt;sub&gt;1&lt;/sub&gt; of Z in a way so that t&lt;sub&gt;1&lt;/sub&gt;'s height being already lower has decreased. (In that case Z's balance factor may be 0.)

There are four situations that might arise. We will describe them as '''Dir1 Dir2''', where '''Dir1''' comes from the set { ''left'', ''right'' } and '''Dir2''' as a balance factor comes from the set { ''left-heavy'' = −1, ''balanced'' = 0, ''right-heavy'' = +1 }.&lt;ref&gt;Thereby, rotations with case ''Balanced'' do not occur with insertions.&lt;/ref&gt;

Situation Dir1 Dir2 denotes:
:'''Z is a Dir1 child of its parent and'''
:: '''Z is Dir2-heavy if Dir2 != Dir1 '''
:: '''Z is not (−Dir2)-heavy if Dir2 == Dir1 '''
i.e.
{| 
|-
| style=&quot;width:1em&quot; | || Right Right || =&gt; Z is a ''right'' || child of its parent X and Z is not ''left-heavy'' || (i.e. BalanceFactor(Z) ≥ 0) || (see figure 4)
|-
| || Left Left || =&gt; Z is a ''left'' || child of its parent X and Z is not ''right-heavy'' || (i.e. BalanceFactor(Z) ≤ 0)
|-
| || Right Left || =&gt; Z is a ''right'' || child of its parent X and Z is ''left-heavy'' || (i.e. BalanceFactor(Z) = −1) || (see figure 5)
|-
| || Left Right || =&gt; Z is a ''left'' || child of its parent X and Z is ''right-heavy'' || (i.e. BalanceFactor(Z) = +1)
|}

The balance violation of case Dir1 == Dir2 is repaired by a simple rotation rotate_(−Dir1) (&lt;tt&gt;rotate_Left&lt;/tt&gt; in figure 4 resp. its mirror &lt;tt&gt;rotate_Right&lt;/tt&gt;).

The case Dir1 != Dir2 is repaired by a double rotation rotate_(−Dir2)(−Dir1) == rotate_Dir1Dir2 (&lt;tt&gt;rotate_RightLeft&lt;/tt&gt; in figure 5 resp. its mirror &lt;tt&gt;rotate_LeftRight&lt;/tt&gt;).

The cost of a rotation, both simple and double, is constant.

===Simple rotation===
Figure 4 shows a Right Right situation. In its upper half, node X has two child trees with a balance factor of &lt;span style=&quot;color:#FF0000;&quot;&gt;+2&lt;/span&gt;. Moreover, the inner child t&lt;sub&gt;23&lt;/sub&gt; of Z (i.e., left child when Z is right child resp. right child when Z is left child) is not higher than its sibling t&lt;sub&gt;4&lt;/sub&gt;. This can happen by a height increase of subtree t&lt;sub&gt;4&lt;/sub&gt; or by a height decrease of subtree t&lt;sub&gt;1&lt;/sub&gt;. In the latter case, also the pale situation where t&lt;sub&gt;23&lt;/sub&gt; has the same height as t&lt;sub&gt;4&lt;/sub&gt; may occur.

The result of the left rotation is shown in the lower half of the figure. Three links (thick edges in figure 4) and two balance factors are to be updated.

As the figure shows, before an insertion, the leaf layer was at level h+1, temporarily at level h+2 and after the rotation again at level h+1. In case of a deletion, the leaf layer was at level h+2, where it is again, when t&lt;sub&gt;23&lt;/sub&gt; and t&lt;sub&gt;4&lt;/sub&gt; were of same height. Otherwise the leaf layer reaches level h+1, so that the height of the rotated tree decreases.

[[File:AVL-simple-left_K.svg|thumb|right|194px|Fig. 4: Simple rotation&lt;br /&gt;''rotate_Left''(''X'',''Z'')]]
;Code snippet of a simple left rotation
{|
|-
| style=&quot;width:4em;&quot; | Input: || X = root of subtree to be rotated left
|-
| || Z = right child of X, Z is right-heavy
|-
| || &amp;nbsp; &amp;nbsp; with height == {{math|Height(LeftSubtree(}}X{{math|))+2}}
|-
|Result: || new root of rebalanced subtree
|}
&lt;source lang=&quot;c&quot; style=&quot;overflow:hidden&quot; line=&quot;1&quot;&gt;
node *rotate_Left(node *X, node *Z) {
    // Z is by 2 higher than its sibling
    t23 = left_child(Z); // Inner child of Z
    right_child(X) = t23;
    if (t23 != null)
        parent(t23) = X;
    left_child(Z) = X;
    parent(X) = Z;
    // 1st case, BalanceFactor(Z) == 0, only happens with deletion, not insertion:
    if (BalanceFactor(Z) == 0) { // t23 has been of same height as t4
        BalanceFactor(X) = +1;   // t23 now higher
        BalanceFactor(Z) = –1;   // t4 now lower than X
    } else { // 2nd case happens with insertion or deletion:
        BalanceFactor(X) = 0;
        BalanceFactor(Z) = 0;
    }
    return Z; // return new root of rotated subtree
}
&lt;/source&gt;

===Double rotation===
Figure 5 shows a Right Left situation. In its upper third, node X has two child trees with a balance factor of &lt;span style=&quot;color:#FF0000;&quot;&gt;+2&lt;/span&gt;. But unlike figure 4, the inner child Y of Z is higher than its sibling t&lt;sub&gt;4&lt;/sub&gt;. This can happen by the insertion of Y itself or a height increase of one of its subtrees t&lt;sub&gt;2&lt;/sub&gt; or t&lt;sub&gt;3&lt;/sub&gt; (with the consequence that they are of different height) or by a height decrease of subtree t&lt;sub&gt;1&lt;/sub&gt;. In the latter case, it may also occur that t&lt;sub&gt;2&lt;/sub&gt; and t&lt;sub&gt;3&lt;/sub&gt; are of same height.

The result of the first, the right, rotation is shown in the middle third of the figure. (With respect to the balance factors, this rotation is not of the same kind as the other AVL single rotations, because the height difference between Y and t&lt;sub&gt;4&lt;/sub&gt; is only 1.) The result of the final left rotation is shown in the lower third of the figure. Five links (thick edges in figure 5) and three balance factors are to be updated.

As the figure shows, before an insertion, the leaf layer was at level h+1, temporarily at level h+2 and after the double rotation again at level h+1. In case of a deletion, the leaf layer was at level h+2 and after the double rotation it is at level h+1, so that the height of the rotated tree decreases.

[[File:AVL-double-rl_K.svg|thumb|right|264px|Fig. 5: Double rotation ''rotate_RightLeft''(''X'',''Z'')&lt;br/&gt;= ''rotate_Right'' around ''Z'' followed by&lt;br/&gt;''rotate_Left'' around ''X'']]
;Code snippet of a right-left double rotation
{|
|-
| style=&quot;width:4em;&quot; | Input: || X = root of subtree to be rotated
|-
| || Z = its right child, left-heavy
|-
| || &amp;nbsp; &amp;nbsp; with height == {{math|Height(LeftSubtree(}}X{{math|))+2}}
|-
|Result: || new root of rebalanced subtree
|}
&lt;source lang=&quot;c&quot; style=&quot;overflow:hidden&quot; line=&quot;1&quot;&gt;
node *rotate_RightLeft(node *X, node *Z) {
    // Z is by 2 higher than its sibling
    Y = left_child(Z); // Inner child of Z
    // Y is by 1 higher than sibling
    t3 = right_child(Y);
    left_child(Z) = t3;
    if (t3 != null)
        parent(t3) = Z;
    right_child(Y) = Z;
    parent(Z) = Y;
    t2 = left_child(Y);
    right_child(X) = t2;
    if (t2 != null)
        parent(t2) = X;
    left_child(Y) = X;
    parent(X) = Y;
    if (BalanceFactor(Y) &gt; 0) { // t3 was higher
        BalanceFactor(X) = –1;  // t1 now higher
        BalanceFactor(Z) = 0;
    } else
        if (BalanceFactor(Y) == 0) {
            BalanceFactor(X) = 0;
            BalanceFactor(Z) = 0;
        } else {
            // t2 was higher
            BalanceFactor(X) = 0;
            BalanceFactor(Z) = +1;  // t4 now higher
        }
    BalanceFactor(Y) = 0;
    return Y; // return new root of rotated subtree
}
&lt;/source&gt;

==Comparison to other structures==
Both AVL trees and red–black (RB) trees are self-balancing binary search trees and they are related mathematically. Indeed, every AVL tree can be colored red–black,&lt;ref&gt;{{cite web|title=AVL tree|trans-title=|periodical=Dictionary of Algorithms and Data Structures|publisher=[[National Institute of Standards and Technology]]|url=https://xlinux.nist.gov/dads/HTML/avltree.html|accessdate=2016-07-02|archiveurl=|archivedate=|last=Paul E. Black|date=2015-04-13|year=|month=|day=|language=|pages=|quote=}}&lt;/ref&gt; but there are RB trees which are not AVL balanced. For maintaining the AVL resp. RB tree's invariants, rotations play an important role. In the worst case, even without rotations, AVL or RB insertions or deletions require {{math|O(log ''n'')}} inspections and/or updates to AVL balance factors resp. RB colors. RB insertions and deletions and AVL insertions require from zero to three [[Tail call|tail-recursive]] rotations and run in [[Amortized analysis|amortized]] {{math|O(1)}} time,&lt;ref&gt;{{harvnb|Mehlhorn|Sanders|2008|pp=165, 158}}&lt;/ref&gt;&lt;ref Name=&quot;Dinesh&quot;&gt;Dinesh P. Mehta, Sartaj Sahni (Ed.) ''Handbook of Data Structures and Applications'' 10.4.2&lt;/ref&gt; thus equally constant on average. AVL deletions requiring {{math|O(log ''n'')}} rotations in the worst case are also {{math|O(1)}} on average. RB trees require storing one bit of information (the color) in each node, while AVL trees mostly use two bits for the balance factor, although, when stored at the children, one bit with meaning «lower than sibling» suffices. The bigger difference between the two data structures is their height limit.

For a tree of size {{math|''n'' &amp;ge; 1}}
*an AVL tree's height is at most
*:&lt;math&gt;
\begin{array}{ll}
h &amp; \leqq \; c \log_2 (n + d) + b \\
&amp; &lt; \; c \log_2 (n + 2) + b
\end{array}
&lt;/math&gt;
:where &lt;math&gt;\varphi := \tfrac{1+\sqrt 5}2 \approx 1.618&lt;/math&gt;&amp;nbsp; the [[golden ratio]], &lt;math&gt;c := \tfrac 1{\log_2 \varphi} \approx 1.440,&lt;/math&gt; &amp;nbsp; &lt;math&gt;b := \tfrac{c}2 \log_2 5 - 2 \approx \; -0.328,&lt;/math&gt; and&amp;nbsp; &lt;math&gt;d:=1+\tfrac{1}{\varphi^4\sqrt{5}} \approx 1.065&lt;/math&gt;.
*an RB tree's height is at most 
*:&lt;math&gt;
\begin{array}{ll}
h &amp; \leqq \; 2\log_2(n+1)
\end{array}
&lt;/math&gt;&amp;nbsp;.&lt;ref&gt;[[Red–black tree#Proof of asymptotic bounds]]&lt;/ref&gt;

AVL trees are more rigidly balanced than RB trees with an [[Asymptotic analysis|asymptotic]] relation {{frac|AVL|RB}}≈0.720 of the maximal heights. For insertions and deletions, Ben Pfaff shows in 79 measurements a relation of {{frac|AVL|RB}} between 0.677 and 1.077 with [[median]] ≈0.947 and [[geometric mean]] ≈0.910.&lt;ref name=&quot;Pfaff_b&quot;&gt;Ben Pfaff: ''Performance Analysis of BSTs in System Software.'' Stanford University 2004.&lt;/ref&gt;

==See also==
*[[tree data structure|Trees]]
*[[Tree rotation]]
*[[WAVL tree]]
*[[Red–black tree]]
*[[Splay tree]]
*[[Scapegoat tree]]
*[[B-tree]]
*[[T-tree]]
*[[List of data structures]]

==References==
&lt;references /&gt;

==Further reading==
* [[Donald Knuth]]. ''[[The Art of Computer Programming]]'', Volume 3: ''Sorting and Searching'', Third Edition. Addison-Wesley, 1997. {{ISBN|0-201-89685-0}}. Pages 458–475 of section 6.2.3: Balanced Trees.

==External links==
{{Wikibooks|Algorithm Implementation|Trees/AVL tree|AVL tree}}
{{Commons category|AVL-trees}}
*{{DADS|AVL Tree|avltree}}

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:AVL Tree}}
[[Category:1962 in computer science]]
[[Category:Binary trees]]
[[Category:Soviet inventions]]
[[Category:Search trees]]</text>
      <sha1>bk69zk71doxdp0sux94pagygswx0snq</sha1>
    </revision>
  </page>
